# New style: separate LLM and Embedding configuration
# This allows using different providers for LLM and Embedding
llm:
  provider: ollama  # "ollama" or "api"
  base_url: ${LLM_API_BASE:http://localhost:11434}
  api_key: ${LLM_API_KEY:}
  model_name: qwen3:4b-instruct-2507-q4_K_M
  temperature: 0.7
  max_tokens: 2048

embedding:
  provider: ollama  # "ollama" or "api"
  base_url: ${EMBEDDING_API_BASE:http://localhost:11434}
  api_key: ${EMBEDDING_API_KEY:}
  model_name: qwen3-embedding:4b
  embedding_dim: 2560

# Legacy style (for backward compatibility)
# If 'llm' and 'embedding' are not specified, these will be used
provider: ollama

ollama:
  base_url: ${OLLAMA_API_BASE:http://localhost:11434}
  api_key: ${OLLAMA_API_KEY:}
  llm_model_name: qwen3:4b-instruct-2507-q4_K_M
  embedding_model_name: qwen3-embedding:4b
  embedding_dim: 2560
  default_temperature: 0.7
  max_tokens: 2048

api:
  base_url: ${OPENAI_API_BASE:https://api.openai.com/v1}
  api_key: ${OPENAI_API_KEY:}
  llm_model_name: gpt-4o-mini
  embedding_model_name: text-embedding-3-small
  embedding_dim: 1536
  default_temperature: 0.7
  max_tokens: 2048
